{
    "componentChunkName": "component---src-templates-tutorial-tsx",
    "path": "/machine-learning/naive-bayes-classifier",
    "result": {"data":{"post":{"slug":"/machine-learning/naive-bayes-classifier","title":"Naive Bayes Classifier","description":"Understand the basics of Naive Bayes using python.","excerpt":"What is Naive Bayes algorithm? Naive bayes is a classification technique which relies on bayes theorem.\nBayes theorem provides an equationâ€¦","body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Naive Bayes Classifier\",\n  \"subtitle\": \"What's so Naive about it?\",\n  \"date\": \"2021-05-11T00:00:00.000Z\",\n  \"lastUpdated\": \"2021-08-07T00:00:00.000Z\",\n  \"description\": \"Understand the basics of Naive Bayes using python.\",\n  \"type\": \"tutorial\",\n  \"category\": \"Machine Learning\",\n  \"image\": \"/og-images/theme-ui-plugin.png?v=1\",\n  \"published\": true\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", {\n    \"id\": \"what-is-naive-bayes-algorithm\"\n  }, \"What is Naive Bayes algorithm?\"), mdx(\"p\", null, \"Naive bayes is a classification technique which relies on bayes theorem.\\nBayes theorem provides an equation describing the relationship of conditional\\nprobabilities of statistical quantities..In simple terms, a Naive Bayes classifier\\nassumes that the presence of a particular feature in a class is unrelated to the\\npresence of any other feature.\"), mdx(\"p\", null, \"Let us understand this with an example. Consider a watermelon. What are the physical\\nfeatures of a watermelon: green, round and huge. Considerably, these features describing\\na watermelon depend on each other for their existence,all of these properties independently\\ncontribute to the probability that this fruit selected at random is a watermelon.This is\\nwhy the term \\u201CNaive\\u201D is used.\"), mdx(\"p\", null, \"Naive Bayes models are easy to build and can be used for initial verifications for large\\ndatasets. Because of its simplicity, it is equally easy to explain the functionality or\\nworking in the background. Before we dive into an example on how to use it in machine\\nlearning. Let;s wrap our heads around the formula.\"), mdx(\"p\", null, \"In Bayesian classification, we\\u2019re interested in finding the probability of a target given\\nsome observed features, which we can write as P(Target | features)\"), mdx(\"p\", null, \"For computation,Bayes theorem can be written as :\"), mdx(\"p\", null, \"P(Target | features) = P(features | Target ) x P(Target) / P(features)\"), mdx(\"p\", null, \"And if we have two targets, say T1, T2, the computations can be represented as :\"), mdx(\"p\", null, \"P(T1 | features) / P(T2 | features) = P(features | T1) x P(T1) / P(features) / P(features | T2) x P(T2) / P(features)\"), mdx(\"h2\", {\n    \"id\": \"when-to-use-it\"\n  }, \"When to use it?\"), mdx(\"p\", null, \"Due to the naive assumption made, these models do not perform well on complex datasets.\\nHowever, below are few advantages of using this classification method\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"They are extremely quick to train and make predictions.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The Explainability score is very high.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Very few parameters to tune and make the model perform better.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"They provide straightforward probabilistic prediction.\")), mdx(\"p\", null, \"These advantages make the Naive Bayesian model a good baselines classification that is\\neasy to set up and takes very less training time. In situations described below, Naive\\nBayes often outperforms other complex models in its simplicity :\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The naive assumptions correlate with the data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Well separated target attributes.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"High dimensional data, where model complexity is less important.\")), mdx(\"h2\", {\n    \"id\": \"code-it-up\"\n  }, \"Code it up!\"), mdx(\"p\", null, \"scikit learn library provides us with three different types of Naive Bayes models:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://scikit-learn.org/stable/modules/naive_bayes.html\"\n  }, \"Gaussian\"), \" : Used for classification, with assumption of normal distribution of features.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://scikit-learn.org/stable/modules/naive_bayes.html\"\n  }, \"Multinomial\"), \" : Used for discrete counts.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://scikit-learn.org/stable/modules/naive_bayes.html\"\n  }, \"Bernoulli\"), \" : Useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with the \\u2018bag of words\\u2019 model where the 1s & 0s are \\u201Cword occurs in the document\\u201D and \\u201Cword does not occur in the document\\u201D respectively.\")), mdx(\"p\", null, \"We consider the dataset, Social media Ads, which has two attributes: age and income\\nand a target with value \", \"[0,1]\", \" describing whether they bought the car or not.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"dataset = pdpp.read_csv('Social_Network_Ads.csv')\\nX = dataset.iloc[:, :-1].values\\ny = dataset.iloc[:, -1].values\\n\")), mdx(\"p\", null, \"Since the two attributes are of diverse values, we need to use StandardScaler.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from sklearn.preprocessing import StandardScaler\\nsc = StandardScaler()\\nX = sc.fit_transform(X)\\n\")), mdx(\"p\", null, \"Splitting the dataset for training and testing.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\\n\")), mdx(\"p\", null, \"Importing the GaussianNB from Naive bayes model\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from sklearn.naive_bayes import GaussianNB\\nclassifier = GaussianNB()\\nclassifier.fit(X_train, y_train)\\n\")), mdx(\"p\", null, \"Let\\u2019s understand how our model performed.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from sklearn.metrics import confusion_matrix, accuracy_score\\ncm = confusion_matrix(y_test, y_pred)\\nprint(cm)\\n\")), mdx(\"p\", null, \"[\", \"[65  3]\", \" \", mdx(\"br\", null), \"\\n\", \"[ 7 25]\", \"]\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"accuracy_score(y_test, y_pred)\\n\")), mdx(\"p\", null, \"0.9\"), mdx(\"p\", null, \"Visualizing training set results of the model will help us understand the model behavior and decision\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from matplotlib.colors import ListedColormap\\nX_set, y_set = sc.inverse_transform(X_train), y_train\\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\\nplt.xlim(X1.min(), X1.max())\\nplt.ylim(X2.min(), X2.max())\\nfor i, j in enumerate(np.unique(y_set)):\\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\\nplt.title('Naive Bayes (Training set)')\\nplt.xlabel('Age')\\nplt.ylabel('Estimated Salary')\\nplt.legend()\\nplt.show()\\n\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"407px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/9d98c9c3e5536e68655d7ab2e83e00b9/b3508/traindata.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"68.359375%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAADyUlEQVQ4y0XSW2hcVRQG4DW5nCRNmjSTzLRpYmInmVznPuecOXPuJwkJU1p8KYgWtQpS47NCEaq0tEIQGtI0t5nJ5GLB24tEUZ/6ICKoSLVFaGibtnnQWKQNWJuxzd6/zKSNCzash83Hv1iLiIhqyUXUTkSvlVTRWnM1fROpsd+3K/37alurPZVB8rh6K/ZV9NY/W99R1lgarPLVd7naagK7DlS2HD59sIpMqqEqqiaBdlGwurqUiNzdVOletZQvl19U7hx/T7mWTGsr4XnjhnwhcStxXloNngveFifiN+IXxFV7XLmljoZuhyci18VpaeXNT15f+fbH5dXJS9PfU3gbbPBWCo153VyBaeORY+LyER1nT6hQMjKiSyqsnI54VsTweBzqVAzDY1EMzqoIZaI48fVbuLr6A5Z/+eIejfp8LiJqbCoXPH8bxq/ctgHLfgzNZpBN9lMqzF4+GWTduRiTsjKzcyaLZURmZFQ2lDaYPm+woQ+Htw5//BxSC6nfqbqkZA8R7WkRBPcDw7gK2wa3bbZlW+CODW6auGlH8OnzAShjASg5DeqcisGMAWM8AjErI5GRmTGvQ02r61TucrURkXdvebl3B7QsVkyq6XikJXF3QMHNwTg+OxqFmFOQmlCQmJWgLZiIzcSgZBXWv9gPM2uuU4XLtZeI6poEwVMEHQdcVRkSCfC+PqCxEQhFcMeM4H60BxePdOHQ2QDkGRHanAZz3oSZM5m9YG+DgsvVXLicZkFo2EloGIwHAiii8TjQ4cdGRwsuBWrwV6AdoyMhHDonQU5LSKRlGDmd2Ys2zIy5TlUlJQ2FpXjKy5t2QMdhSCa3sWAQPBQCb2oCauuAriA+fyUBPafDmbeQnFNhLTr/J0QqVbjtuv2C0PjAMK48GXmLyzLnhsF5Tw/nosh5WytnwQBHTOQfvRDi/WMRbs9b3JmSuTkeZfZSP8yM8Qc9qbrCyP8Yxm8FEIbBoSiAJAG9vSj23d1gugZIGt5+tQXPTPigZ5OITYWRnBHhLBWX8udT0OMuK9t/T9N+/teysKmqD/N+fz7v8+XzoVDxbUYi+ceGuXnf0jaTJ1vz7ZO9+cSUlI9NxfPJtPrQylpMn9bXnoJlLYJQMurzhb+LRvuuSFLnea9XWfP7O77avTs463ZLGwdTBybb/cpkrDs8NDPU3vtSjyKeErv1U1pP19FOJfFBwi+eFtsJjBXFmtJSwrFjtKHrdF1RCKpKl9vaiLvdhM5OuuY4hW9e5N6lyDsSvXH3OGljGg1MD9AIGyHxjEjSGYn+A6Zk1yrzSKrlAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Training data\",\n    \"title\": \"Training data\",\n    \"src\": \"/static/9d98c9c3e5536e68655d7ab2e83e00b9/b3508/traindata.png\",\n    \"srcSet\": [\"/static/9d98c9c3e5536e68655d7ab2e83e00b9/85b06/traindata.png 256w\", \"/static/9d98c9c3e5536e68655d7ab2e83e00b9/b3508/traindata.png 407w\"],\n    \"sizes\": \"(max-width: 407px) 100vw, 407px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-py\"\n  }, \"from matplotlib.colors import ListedColormap\\nX_set, y_set = sc.inverse_transform(X_test), y_test\\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\\nplt.xlim(X1.min(), X1.max())\\nplt.ylim(X2.min(), X2.max())\\nfor i, j in enumerate(np.unique(y_set)):\\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\\nplt.title('Naive Bayes (Test set)')\\nplt.xlabel('Age')\\nplt.ylabel('Estimated Salary')\\nplt.legend()\\nplt.show()\\n\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"407px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/fec9fd6956ebe6c60473c05e30e96e26/b3508/testdata.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"68.359375%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAADqklEQVQ4yz2SW2gcVRzG/7vJjt1u04TcmjSxko2NmzTt3mZ3z87MOWdmQ4VQxJcIWkSKVINFEQXBy4OlNA9FEO0t6Wa7OykKWp+sF/CloEhBQaotQuIlwTzY+FICjcm03TmfzMbmwMfhwOHH//t/HxER7aQQ0SARPR+O0nJfjL7NxjKTmdgz5sCOnT3bh6mThqM90dGWh1uGor3R4Zb+lkTbnrahnkM9LdZLLEYWxShKMdJoO+2PxZqIqD1B29oXJfvyymH21+S7bL54wVxIzvEFc4Yv8rPFxcwHmaXcmdyivCD/CO782fyfTsX5LXUuvfDK5cn57378fPHc1elrlNwEdnRv0zo9SyxA2LjnCFyfsDD1poGxCkfu0xLkJQeOK2HPOZCuBHc5bFciXc7gja9fx82lH3Dl5y9u06l4PEREnb0RresO578o2wakfR+m7YNJf+mQ6U+9lvMPzpg+v+T4dlX4tmv7juv4vCr8/GzeL82N1Z/45EmMu+N/UywcbiOitn5Na1/j/CZsG0pKv+7YqJdsQJSAHMc3T+fAKwayFwsolgsQNQHHdRqSNelbVRNm2VyhSCj0CBF174pEureAtu3D4rjLDdxycsHE8KXAtQmGx2csGC6HWTEa0EC8JnwnWEVFrtBDodAuImrt1bSuBtBxoBjzMTIClUmjHh8AUmmsCQboeVx+TkfeNcHKeViuwMHzRZQ+zPryUgliVqyQFgr1Bc3p07SOLaBp+ooxQAiAc4ALrFsMd4pZqFQKr54wUPjYQalmB3Zh12SwV4iKWKFoONwRhNIVifRuWbYsH4xBJZNALodGUFl98z2axWcvWtA/kihVJYyLJoKQtoAYHw+63bpb0zrXOL/RAApRV5wrZZpKGYZSxaJSyaSqZzMKBUtdndCVUbOU4zqKV7kyKsYmcFbcov9Pa2D5X85/RaM2UkFKQNcRTArLalhXYyXAsnH85VGMVLMQVQtm1YRVtRCEIirinwfArvbm5t23TfOnu1Jiw7LWPcY8L532vGLR83Td20gmvfvS3lgVxsZTU1lPr5qeLHNPVmSgdVERvjVtLT8ANvdrWvhUPJ78PpXadyOfHzqdSLBlIR796sCB/eWOjtyq4APn43vZmexjyZHpwuDIswmWPZ5NpN5JDQ8eHmTsPbZXP6EPEny/QdzR1EQ4coRWTZN+Z4xw7BhdLxRISUlIJGi+UAi+daP2Nu15ax9NrrxA7H1GxmmDjt47SvpJnXInc/Qf3KPgL7y4LOkAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Test Data\",\n    \"title\": \"Test Data\",\n    \"src\": \"/static/fec9fd6956ebe6c60473c05e30e96e26/b3508/testdata.png\",\n    \"srcSet\": [\"/static/fec9fd6956ebe6c60473c05e30e96e26/85b06/testdata.png 256w\", \"/static/fec9fd6956ebe6c60473c05e30e96e26/b3508/testdata.png 407w\"],\n    \"sizes\": \"(max-width: 407px) 100vw, 407px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h2\", {\n    \"id\": \"improving-naive-bayes\"\n  }, \"Improving Naive Bayes\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Transform continious features into normal distribution.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"USe Laplace Estimator in cases of zero frequency problem.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Focus on pre processing and feature selection to optimize the results of Naive Bayes.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Do not spend time applying bagging boosting or ensemble method on Naive Bayes, as it has no variance to minimize.\")), mdx(\"p\", null, \"I hope you found this article useful. To follow the code, I have created a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/amankalra172/Classification/tree/master/Naive%20Bayes\"\n  }, \"sample notebook\"), \". Feel free to make a copy of the notebook for your experiments.\\nDo let me know if you have any questions!\"));\n}\n;\nMDXContent.isMDXComponent = true;","seoLastUpdated":"2021-08-07T00:00:00.000Z","lastUpdated":"Aug 07, 2021","seoDate":"2021-05-11T00:00:00.000Z","yearDate":"2021","date":"May 11, 2021","subtitle":"What's so Naive about it?","timeToRead":2,"image":"/og-images/theme-ui-plugin.png?v=1","category":{"name":"Machine Learning","slug":"/machine-learning"},"parent":{"parent":{"relativePath":"2020-08-02-using-NLP-to-detect-Fake-news copy/index.mdx"}},"tableOfContents":{"items":[{"url":"#what-is-naive-bayes-algorithm","title":"What is Naive Bayes algorithm?"},{"url":"#when-to-use-it","title":"When to use it?"},{"url":"#code-it-up","title":"Code it up!"},{"url":"#improving-naive-bayes","title":"Improving Naive Bayes"}]}}},"pageContext":{"slug":"/machine-learning/naive-bayes-classifier"}},
    "staticQueryHashes": ["2299006781","3050858678","4184542181","712324210"]}